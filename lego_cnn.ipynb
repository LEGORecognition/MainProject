{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540dff9",
   "metadata": {},
   "source": [
    "### Dataset preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"images\"                 # Directory containing PNG images\n",
    "XML_DIR = \"annotations\"              # Directory containing XML annotations\n",
    "OUTPUT_DIR = \"cnn_dataset\"           # Directory to save the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique color classes from XML annotations\n",
    "color_classes = set()\n",
    "for xml_file in glob.glob(os.path.join(XML_DIR, \"*.xml\")):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for obj in root.findall('object'):\n",
    "        color = obj.find('color').text\n",
    "        if color:\n",
    "            color_classes.add(color)\n",
    "\n",
    "color_classes = sorted(list(color_classes))\n",
    "\n",
    "print(f\"Found {len(color_classes)} unique brick classes: {color_classes}\")\n",
    "\n",
    "# Create directories for each color class\n",
    "for color in color_classes:\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, 'train', color), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, 'test', color), exist_ok=True) \n",
    "print(f\"Successfully created directories for each color class in {OUTPUT_DIR}/train and {OUTPUT_DIR}/test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc9fcc",
   "metadata": {},
   "source": [
    "**Note:** If you already have the data set, skip next 2 cells. If you want to generate yourself I would suggest starting with less amount of images or rewriting the function provided below to multithreading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to multithreading if possible (one one thread it took 12h to run half of the dataset lmao)\n",
    "def extract_bricks_from_image(file_id, dataset_type):\n",
    "    try:\n",
    "        tree = ET.parse(os.path.join(XML_DIR, f\"{file_id}.xml\"))\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            try:\n",
    "                # Get color\n",
    "                color_class = obj.find('color').text\n",
    "                if not color_class:\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box coordinates\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = float(bndbox.find('xmin').text)\n",
    "                ymin = float(bndbox.find('ymin').text)\n",
    "                xmax = float(bndbox.find('xmax').text)\n",
    "                ymax = float(bndbox.find('ymax').text)\n",
    "                \n",
    "                # Load the image\n",
    "                img_path = os.path.join(IMAGE_DIR, f\"{file_id}.png\")\n",
    "                if not os.path.exists(img_path):\n",
    "                    continue\n",
    "                    \n",
    "                img = Image.open(img_path)\n",
    "                image_width, image_height = img.size\n",
    "                \n",
    "                # Ensure coordinates are within image bounds\n",
    "                xmin = max(0, int(xmin))\n",
    "                ymin = max(0, int(ymin))\n",
    "                xmax = min(image_width, int(xmax))\n",
    "                ymax = min(image_height, int(ymax))\n",
    "                \n",
    "                # Check if the box is valid (has positive width and height)\n",
    "                if xmax <= xmin or ymax <= ymin:\n",
    "                    continue\n",
    "                \n",
    "                # Crop the image to the bounding box\n",
    "                brick_img = img.crop((xmin, ymin, xmax, ymax))\n",
    "                \n",
    "                # Create a unique filename for the brick\n",
    "                output_filename = f\"{file_id}_{xmin}_{ymin}_{xmax}_{ymax}.png\"\n",
    "                output_path = os.path.join(OUTPUT_DIR, dataset_type, color_class, output_filename)\n",
    "                \n",
    "                # Save the cropped image\n",
    "                brick_img.save(output_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing object in {file_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b81b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this if you already have the dataset\n",
    "# This will take a lot of time lmao\n",
    "\n",
    "# Get all image files\n",
    "image_files = sorted(glob.glob(os.path.join(IMAGE_DIR, \"*.png\")))\n",
    "print(f\"Found {len(image_files)} image files\")\n",
    "\n",
    "# Randomly split into train and validation sets (80:20)\n",
    "random.seed(42)  # for reproducibility\n",
    "random.shuffle(image_files)\n",
    "split_idx = int(0.8 * len(image_files))\n",
    "train_images = image_files[:split_idx]\n",
    "test_images = image_files[split_idx:]\n",
    "\n",
    "print(f\"Training images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(test_images)}\")\n",
    "\n",
    "for img_path in train_images:\n",
    "    file_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    extract_bricks_from_image(file_id, 'train')\n",
    "\n",
    "for img_path in test_images:\n",
    "    file_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    extract_bricks_from_image(file_id, 'test')\n",
    "\n",
    "print(\"Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c333b",
   "metadata": {},
   "source": [
    "### Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e816de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used this notebook as a reference for creating this model: \n",
    "# https://github.com/deepsense-ai/Keras-PyTorch-AvP-transfer-learning/blob/master/PyTorch-ResNet50.ipynb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.__version__, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transorms for data augmentation and normalization\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create data loaders with transforms\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    datasets.ImageFolder(os.path.join(OUTPUT_DIR, 'train'), data_transforms['train']),\n",
    "    'test': \n",
    "    datasets.ImageFolder(os.path.join(OUTPUT_DIR, 'test'), data_transforms['test'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=True, num_workers=4),\n",
    "    'test':\n",
    "    torch.utils.data.DataLoader(image_datasets['test'],\n",
    "                                batch_size=32,\n",
    "                                shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ResNet50 model and put it on the GPU\n",
    "model = models.resnet50().to(device)\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, len(color_classes))).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf251bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in tqdm(dataloaders['train'], desc=f'Epoch {epoch+1}/{epochs} - training (batches)'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.detach() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(image_datasets['train'])\n",
    "        epoch_acc = running_corrects.float() / len(image_datasets['train'])\n",
    "        print('train loss: {:.4f}, acc: {:.4f}'.format(epoch_loss.item(), epoch_acc.item()))\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(dataloaders['test'], desc=f'Epoch {epoch+1}/{epochs} - validation'):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            val_loss = running_loss / len(image_datasets['test'])\n",
    "            val_acc = running_corrects.float() / len(image_datasets['test'])\n",
    "            print(f'val loss: {val_loss.item():.4f}, acc: {val_acc.item():.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                print(f\"New best validation loss: {best_val_loss.item():.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"Early stopping counter: {patience_counter}/{patience}\")\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            torch.save(model.state_dict(), \"models/weights_train.h5\")\n",
    "            print(f\"Saved model at epoch {epoch}\")\n",
    "\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and save the weights\n",
    "\n",
    "# state_dict = torch.load('models/weights.h5')\n",
    "# model.load_state_dict(state_dict)\n",
    "# print(\"Loaded model weights from file.\")\n",
    "\n",
    "model_trained = train_model(model, criterion, optimizer)\n",
    "torch.save(model_trained.state_dict(), 'models/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e787bf",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "- actually train the model\n",
    "- add a function to test the model on a single image and plot the results\n",
    "- build a pipeline with yolo and cnn for full functionality\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
